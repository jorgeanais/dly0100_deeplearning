{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diseccionando una red neuronal \n",
    "\n",
    "**Autor:**\n",
    "> Profesor Jorge Anais Vilchez  \n",
    "> Abril 2024  \n",
    "\n",
    "En este notebook se muestra como programar una red neuronal a un nivel menos abstracto de lo que hemos revisado, a fin de que podemas ilustrar de mejor manera el funcionamiento de esta. Para ello utilizaremos Pytorch, una librería de Python que nos permite trabajar con redes neuronales de manera sencilla.\n",
    "\n",
    "Finalmente se muestra una manera más sencilla y usual de programar una red neuronal utilizando Pytorch, es decir, algunos detalles de la implementación quedan ocultos.\n",
    "\n",
    "Este material se basa en la clases del profesor Jorge Pérez publicada en YT (https://youtu.be/y6aD4WG-rOw?si=fyNnilzbXLdWPFyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Fijamos una semilla para que los experimentos sean reproducibles\n",
    "t_cg = torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de algunas funciones de activación y función de error, y la respectiva derivada\n",
    "\n",
    "Sigmoide:\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + e^{-t}}; \\quad \\quad \\quad \\frac{d}{dt} \\sigma(t)= \\sigma(t)(1-\\sigma(t))$$\n",
    "\n",
    "Tangente hiperbólica:\n",
    "\n",
    "$$\\tanh(t) = \\frac{e^t - e^{-t}}{e^t + e^{-t}}; \\quad \\quad \\quad \\frac{d}{dt} \\tanh(t)= 1 - \\tanh^2(t)$$\n",
    "\n",
    "Entropía cruzada binaria:\n",
    "\n",
    "$$ \\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y_i}) - (1 - y_i) \\log(1 - \\hat{y_i}) \\right] \\quad \\quad \\quad \\frac{\\partial}{\\partial\\hat{y}} \\mathcal{L} = \\frac{1}{N}(\\hat{y} - y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(T: torch.Tensor):\n",
    "    \"\"\"Función de activación sigmoide\"\"\"\n",
    "    return torch.reciprocal(1 + torch.exp(-1 * T))\n",
    "\n",
    "\n",
    "def tanh(T: torch.Tensor):\n",
    "    \"\"\"Función de activación tangente hiperbólica.\"\"\"\n",
    "    E = torch.exp(T)\n",
    "    e = torch.exp(-1 * T)\n",
    "    return (E - e) * torch.reciprocal(E + e)\n",
    "\n",
    "\n",
    "def bi_cross_ent_loss(y_pred, y, safe=True, epsilon=1e-7):\n",
    "    \"\"\"Función de pérdida de entropía cruzada binaria\"\"\"\n",
    "    \n",
    "    N = y.size()[0]  # tamaño del batch\n",
    "    \n",
    "    # Asegura que no haya valores indefinidos.\n",
    "    if safe:\n",
    "        y_pred = y_pred.clamp(epsilon, 1 - epsilon) \n",
    "    \n",
    "    B = (1-y) * torch.log(1 - y_pred) + y * torch.log(y_pred)\n",
    "    return -1/N * torch.sum(B)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de la red neuronal\n",
    "\n",
    "Consideremos la siguiente red con dos capas ocultas\n",
    "![Imagen](./imagenes/red_manual/esquema.png)\n",
    "\n",
    "Escribimos el paso de foward y backward (aquí necesitamos también las derivadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(torch.nn.Module):\n",
    "    def __init__(self, d0=300, d1=200, d2=300, init_v=1):\n",
    "        \"\"\"\n",
    "        Crea la red FFNN con 2 capas ocultas y una capa de salida.\n",
    "        d0: dimensión de la capa de entrada\n",
    "        d1: dimensión de la primera capa oculta\n",
    "        d2: dimensión de la segunda capa oculta\n",
    "        init_v: multiplicador valor inicial de los pesos\n",
    "        \"\"\"\n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        # Crea los tensores como parámetros\n",
    "        self.W1 = torch.nn.Parameter(torch.randn(d0,d1) * init_v) \n",
    "        self.b1 = torch.nn.Parameter(torch.zeros(d1))\n",
    "        self.W2 = torch.nn.Parameter(torch.randn(d1,d2) * init_v)\n",
    "        self.b2 = torch.nn.Parameter(torch.zeros(d2))\n",
    "        self.U  = torch.nn.Parameter(torch.randn(d2,1) * init_v)\n",
    "        self.c  = torch.nn.Parameter(torch.zeros(1))\n",
    "            \n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Calcula la pasada hacia adelante\n",
    "        u1 = x @ self.W1 + self.b1\n",
    "        h1 = tanh(u1)\n",
    "        u2 = h1 @ self.W2 + self.b2\n",
    "        h2 = sig(u2)\n",
    "        u3 = h2 @ self.U + self.c\n",
    "        y_pred = sig(u3)\n",
    "        \n",
    "        self._cache = [u1, u2]  # Guaradamos temporalmente los valores de u1 y u2\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    # Backpropagation \n",
    "    def backward(self, x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor):\n",
    "        \n",
    "        u1, u2 = self._cache  # recuperamos los valores de u1 y u2\n",
    "\n",
    "        # tamaño del batch\n",
    "        b = x.size()[0]\n",
    "        \n",
    "        # Estas son derivadas calculadas a mano\n",
    "        dL_du3 = (1/b) * (y_pred - y)\n",
    "        dL_dU  = sig(u2).t() @ dL_du3\n",
    "        dL_dc  = torch.sum(dL_du3, 0)\n",
    "        dL_dh2 = dL_du3 @ self.U.t()\n",
    "        dL_du2 = dL_dh2 * sig(u2) * (1 - sig(u2)) \n",
    "        dL_dW2 = tanh(u1).t() @ dL_du2\n",
    "        dL_db2 = torch.sum(dL_du2, 0)\n",
    "        dL_dh1 = dL_du2 @ self.W2.t()\n",
    "        dL_du1 = dL_dh1 * (1 - tanh(u1) * tanh(u1))\n",
    "        dL_dW1 = x.t() @ dL_du1\n",
    "        dL_db1 = torch.sum(dL_du1, 0)\n",
    "                \n",
    "        # Registra los valores de gradientes en cada tensor (que nos interesa)\n",
    "        grads = [dL_dW1, dL_db1, dL_dW2, dL_db2, dL_dU, dL_dc]\n",
    "        params = [self.W1, self.b1, self.W2, self.b2, self.U, self.c]\n",
    "        for p, g in zip(params, grads):\n",
    "            p.grad = g \n",
    "            \n",
    "    def num_parameters(self):\n",
    "        total = 0\n",
    "        for p in self.parameters():\n",
    "            total += p.numel()\n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de Datos\n",
    "\n",
    "Aquí se genera un conjunto de datos aleatorios para entrenar la red neuronal. La clase `Dataset` define un conjunto de datos y la clase `DataLoader` permite cargar los datos en lotes.\n",
    "Es una manera muy eficiente, puede cargar en paralelo en la CPU y GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class RandomDataSet(Dataset):\n",
    "    def __init__(self, N: int, f: int):\n",
    "        \"\"\"Genera datos al azar. N indica el numero de ejemplos y f el número de caracteristicas (features)\"\"\"\n",
    "        R_N_f = torch.rand(N,f)\n",
    "        self.X = torch.bernoulli(R_N_f)\n",
    "        R_N_1 = torch.rand(N,1)\n",
    "        self.Y = torch.bernoulli(R_N_1)\n",
    "        self.num_features = f\n",
    "        \n",
    "    # Debemos definir __len__ para retornar el tamaño del dataset\n",
    "    def __len__(self):\n",
    "        return self.X.size()[0]\n",
    "\n",
    "    # Debemos definir __getitem__ para retornar el i-ésimo \n",
    "    # ejemplo en nuestro dataset.\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.Y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucle de entrenamiento\n",
    "\n",
    "Ahora debemos utilizar nuestros datos para entrenar la red. Para ello, necesitamos un bucle que haga lo siguiente:\n",
    "\n",
    "1. Tomar un lote o subconjunto de nuestros datos de entrenamiento\n",
    "2. Calcular la salida de la red (forward).\n",
    "3. Calcular la función de error.\n",
    "4. Calcular las derivadas de los pesos (backward).\n",
    "5. Actualizar los pesos utilizando el descenso del gradiente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_FFNN(\n",
    "    dataset: Dataset,\n",
    "    batch_size: int,   # tamaño del lote\n",
    "    d1: int,  # número de neuronas en la capa 1\n",
    "    d2: int,  # número de neuronas en la capa 2\n",
    "    lr: float,  # tasa de aprendizaje\n",
    "    N: int, # numero de ejemplos\n",
    "    epochs: int,\n",
    "    run_in_GPU: bool=True,\n",
    "    reports_every: int=1,\n",
    "    init_v: float = 1.,\n",
    "):\n",
    "    \n",
    "    # Define un tipo para los tensores según si correrá en la GPU o no\n",
    "    device = 'cuda' if run_in_GPU else 'cpu'\n",
    "        \n",
    "    # d0 es la cantidad de features    \n",
    "    d0 = dataset.num_features\n",
    "    \n",
    "    # Crea la red\n",
    "    red = FFNN(d0, d1, d2, init_v)\n",
    "        \n",
    "    # Pasa la red al dispositivo elegido\n",
    "    red.to(device)\n",
    "    \n",
    "    # Muestra la cantidad de parámetros\n",
    "    print(f\"Cantidad de parámetros: {red.num_parameters()}\")\n",
    "    \n",
    "    # Crea un dataloader desde el dataset\n",
    "    data = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    \n",
    "    # Comienza el entrenamiento\n",
    "    tiempo_epochs = 0\n",
    "    for e in range(1, epochs + 1):    \n",
    "        inicio_epoch = time.process_time()\n",
    "        \n",
    "        for (x, y) in data:         \n",
    "            # Asegura de pasarlos a la GPU si fuera necesario\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Computa la pasada hacia adelante (forward)\n",
    "            y_pred = red.forward(x)\n",
    "            \n",
    "            # Computa la función de pérdida\n",
    "            L = bi_cross_ent_loss(y_pred, y)\n",
    "            \n",
    "            # Computa los gradientes hacia atrás (backpropagation)\n",
    "            red.backward(x, y, y_pred)\n",
    "            \n",
    "            \n",
    "            # Descenso de gradiente para actualizar los parámetros\n",
    "            for p in red.parameters():\n",
    "                p.data = p.data - lr * p.grad\n",
    "            \n",
    "        tiempo_epochs += time.process_time() - inicio_epoch\n",
    "        \n",
    "        # Reporta el acierto cada \"reports_every\" cantidad de épocas\n",
    "        if e % reports_every == 0:\n",
    "            \n",
    "            # Calcula la certeza de las predicciones sobre todo el conjunto\n",
    "            X = dataset.X.to(device)\n",
    "            Y = dataset.Y.to(device)\n",
    "\n",
    "            # Predice usando la red\n",
    "            Y_PRED = red.forward(X)\n",
    "            \n",
    "            # Calcula la pérdida de todo el conjunto\n",
    "            L_total = bi_cross_ent_loss(Y_PRED, Y)\n",
    "\n",
    "            # Elige una clase dependiendo del valor de Y_PRED\n",
    "            Y_PRED_BIN = (Y_PRED >= 0.5).float()\n",
    "\n",
    "            correctos = torch.sum(Y_PRED_BIN == Y).item()\n",
    "            acc = (correctos / N) * 100\n",
    "\n",
    "            sys.stdout.write(\n",
    "                f\"Epoch:{e:03d} Acc:{acc:.2f} Loss:{L_total:.4f} Tiempo/epoch:{tiempo_epochs/e:.3f}s\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de como ejecutar nuestra red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000 # numero de ejemplos\n",
    "f = 300 # numero de features\n",
    "\n",
    "dataset = RandomDataSet(N,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de parámetros: 211101\n",
      "Epoch:001 Acc:56.16 Loss:2.5187 Tiempo/epoch:0.155s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:002 Acc:59.92 Loss:1.9686 Tiempo/epoch:0.133s\n",
      "Epoch:003 Acc:67.20 Loss:1.3564 Tiempo/epoch:0.126s\n",
      "Epoch:004 Acc:66.58 Loss:1.2281 Tiempo/epoch:0.122s\n",
      "Epoch:005 Acc:76.20 Loss:0.7849 Tiempo/epoch:0.119s\n",
      "Epoch:006 Acc:73.78 Loss:0.7616 Tiempo/epoch:0.117s\n",
      "Epoch:007 Acc:88.18 Loss:0.4127 Tiempo/epoch:0.116s\n",
      "Epoch:008 Acc:90.40 Loss:0.3360 Tiempo/epoch:0.114s\n",
      "Epoch:009 Acc:92.70 Loss:0.2717 Tiempo/epoch:0.114s\n",
      "Epoch:010 Acc:94.30 Loss:0.2196 Tiempo/epoch:0.113s\n",
      "Epoch:011 Acc:93.32 Loss:0.2264 Tiempo/epoch:0.112s\n",
      "Epoch:012 Acc:95.28 Loss:0.1769 Tiempo/epoch:0.112s\n",
      "Epoch:013 Acc:97.00 Loss:0.1309 Tiempo/epoch:0.112s\n",
      "Epoch:014 Acc:97.22 Loss:0.1302 Tiempo/epoch:0.113s\n",
      "Epoch:015 Acc:98.08 Loss:0.1020 Tiempo/epoch:0.115s\n",
      "Epoch:016 Acc:98.52 Loss:0.0894 Tiempo/epoch:0.115s\n",
      "Epoch:017 Acc:98.76 Loss:0.0802 Tiempo/epoch:0.114s\n",
      "Epoch:018 Acc:99.04 Loss:0.0711 Tiempo/epoch:0.114s\n",
      "Epoch:019 Acc:98.98 Loss:0.0729 Tiempo/epoch:0.114s\n",
      "Epoch:020 Acc:99.24 Loss:0.0595 Tiempo/epoch:0.114s\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "loop_FFNN(\n",
    "    dataset,\n",
    "    batch_size=64,  # 10\n",
    "    d1=300,\n",
    "    d2=400,\n",
    "    N=N,\n",
    "    epochs=epochs,\n",
    "    run_in_GPU=True,\n",
    "    lr=0.06,\n",
    "    init_v=1.,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preguntas\n",
    "\n",
    "1. **¿Qué pasa al aumentar el tamaño del lote (`batch_size`) de 10 a 64?**   \n",
    "R: El trabajo total se mantiene constante (la cantidad de multiplicaciones es la misma). El tiempo de ejecución disminuye porque se procesan de manera más óptima (\"la red está mirando más valores al mismo tiempo\").\n",
    "\n",
    "2. **Analice el tiempo por epoca en los siguientes casos**:\n",
    "\n",
    "|              | CPU    | GPU    | CPU     | GPU     | CPU     | GPU     |\n",
    "|--------------|--------|--------|---------|---------|---------|---------|\n",
    "| epochs       | 20     | 20     | 20      | 20      | 20      | 20      |\n",
    "| batch size   | 128    | 128    | 128     | 128     | 128     | 128     |\n",
    "| d1           | 500    | 500    | 1000    | 1000    | 2000    | 2000    |\n",
    "| d2           | 800    | 800    | 2000    | 2000    | 2000    | 2000    |\n",
    "| tiempo/epoca | 0.45s  | 0.10s  | 4.45s   | 0.10s   | 19.22s  | 0.10s   |\n",
    "| N params     | 552101 | 552101 | 2305001 | 2305001 | 4606001 | 4606001 |\n",
    "| acc          | 99.38% | 99.30% | --      | --      | --      | --      |\n",
    "\n",
    "La mejora en tiempo que logra la CPU con respecto a la CPU es más evidente cuando la red es más grande. Esto se debe a que hay un tiempo fijo de carga de datos entre la CPU y la GPU.\n",
    "\n",
    "3. **Los parámetros se inicializan con una distribución normal estándar, es decir con valores alrededor de cero con una desviación estándar de 1. ¿Qué pasa agrandamos estos números (`init_v=2`)?**   \n",
    "R: notar que la acc baja (~50%) y la función de error se dispara (nan). Notar que tenemos multiplicaciones de matrices que se multiplican por matrices. Esto es muy sensible cuando los números de los parámetros son más grandes que 1, y podrían crecer exponencialmente. Esto podría causar que la red sea inestable. Por otra parte, si se usan numeros muy chiquitos (`init_v=0.001`) la red no es capaz de aprender y su accuracy se queda cercana a ~50%.\n",
    "\n",
    "4. **¿Qué pasa si aumentamos el tamaño del batch y dejamos la cantidad de épocas fijas?**  \n",
    "R: La cantidad de veces que realizas el \"descenso del grandiente\" es menor. \n",
    "\n",
    "5. **Comentario: En vez de usar red.backward(), podemos dejar que pytorch lo haga por nosotros, incluyendo el cálculo de las derivadas.**\n",
    "Comentar la línea `red.backward(x, y, y_pred)` y reemplazar por\n",
    "```python\n",
    "for p in red.parameters():\n",
    "    if p.grad is not None:\n",
    "        p.grad.zero_()\n",
    "\n",
    "L.backward()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red neuronal estilo Pytorch\n",
    "\n",
    "Pytorch ya implementa varias capas de redes predefinidas. Por ejemplo, las redes fully connected están implementadas en `torch.nn.Linear`. Además, Pytorch ya implementa varias funciones de activación, como la sigmoide, tangente hiperbólica, etc.\n",
    "\n",
    "Al hacerlo de esta manera, pytorch crea los parámetros de manera automática y los inicializa con buenas heurísticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red estilo pytorch\n",
    "class FFNN(torch.nn.Module):\n",
    "    def __init__(self, d0: int = 300, d1: int = 200, d2: int = 300):\n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        # Definimos capas (automáticamente se registran como parametros)\n",
    "        self.fc1 = torch.nn.Linear(d0, d1, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(d1, d2, bias=True)\n",
    "        self.fc3 = torch.nn.Linear(d2,  1, bias=True)\n",
    "    \n",
    "    # Computa la pasada hacia adelante\n",
    "    def forward(self, x: torch.Tensor):\n",
    "    \n",
    "        u1 = self.fc1(x)\n",
    "        h1 = torch.tanh(u1)\n",
    "        u2 = self.fc2(h1)\n",
    "        h2 = torch.sigmoid(u2)\n",
    "        u3 = self.fc3(h2)\n",
    "        y_pred = torch.sigmoid(u3)\n",
    "            \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_FFNN_pytorch_style(\n",
    "    dataset: Dataset,\n",
    "    batch_size: int,   # tamaño del lote\n",
    "    d1: int,  # número de neuronas en la capa 1\n",
    "    d2: int,  # número de neuronas en la capa 2\n",
    "    lr: float,  # tasa de aprendizaje\n",
    "    N: int, # numero de ejemplos\n",
    "    epochs: int,\n",
    "    run_in_GPU: bool=True,\n",
    "    reports_every: int=1,\n",
    "):\n",
    "    \n",
    "    # Define un tipo para los tensores según si correrá en la GPU o no\n",
    "    device = 'cuda' if run_in_GPU else 'cpu'\n",
    "        \n",
    "    # d0 es la cantidad de características (features)    \n",
    "    d0 = dataset.num_features\n",
    "    \n",
    "    # Crea la red\n",
    "    red = FFNN(d0, d1, d2)\n",
    "        \n",
    "    # Pasa la red al dispositivo elegido\n",
    "    red.to(device)\n",
    "                \n",
    "    # Muestra la cantidad de parámetros\n",
    "    print('Número de parámetros de la red:', red)\n",
    "    \n",
    "    # Crea un dataloader desde el dataset\n",
    "    data = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    \n",
    "    # Crea un optimizador para el descenso de gradiente\n",
    "    optimizador = torch.optim.SGD(red.parameters(), lr)\n",
    "    \n",
    "    # Define una perdida\n",
    "    perdida = torch.nn.BCELoss()\n",
    "    \n",
    "    # Comienza el entrenamiento\n",
    "    tiempo_epochs = 0\n",
    "    for e in range(1,epochs+1):    \n",
    "        inicio_epoch = time.process_time()\n",
    "        \n",
    "        for (x,y) in data:         \n",
    "            # Asegura de pasarlos a la GPU si fuera necesario\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Computa la pasada hacia adelante (forward)\n",
    "            y_pred = red.forward(x)\n",
    "            \n",
    "            # Computa la función de pérdida\n",
    "            L = perdida(y_pred,y)\n",
    "            \n",
    "            # Computa los gradientes hacia atrás (backpropagation)\n",
    "            L.backward()\n",
    "            \n",
    "            # Descenso de gradiente para actualizar los parámetros\n",
    "            optimizador.step()\n",
    "            \n",
    "            # Limpia los gradientes\n",
    "            optimizador.zero_grad()\n",
    "            \n",
    "        tiempo_epochs += time.process_time() - inicio_epoch\n",
    "        \n",
    "        if e % reports_every == 0:\n",
    "            # Calcula la certeza de las predicciones sobre todo el conjunto\n",
    "            X = dataset.X.to(device)\n",
    "            Y = dataset.Y.to(device)\n",
    "\n",
    "            # Predice usando la red\n",
    "            Y_PRED = red.forward(X)\n",
    "            \n",
    "            # Calcula la pérdida de todo el conjunto\n",
    "            L_total = perdida(Y_PRED, Y)\n",
    "\n",
    "            # Elige una clase dependiendo del valor de Y_PRED\n",
    "            Y_PRED_BIN = (Y_PRED >= 0.5).float()\n",
    "\n",
    "            correctos = torch.sum(Y_PRED_BIN == Y).item()\n",
    "            acc = (correctos / N) * 100\n",
    "\n",
    "            sys.stdout.write(\n",
    "                f\"Epoch:{e:03d} Acc:{acc:.2f} Loss:{L_total:.4f} Tiempo/epoch:{tiempo_epochs/e:.3f}s\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de parámetros de la red: FFNN(\n",
      "  (fc1): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=300, out_features=500, bias=True)\n",
      "  (fc3): Linear(in_features=500, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:001 Acc:48.78 Loss:0.9442 Tiempo/epoch:0.177s\n",
      "Epoch:002 Acc:51.22 Loss:0.7029 Tiempo/epoch:0.165s\n",
      "Epoch:003 Acc:51.22 Loss:0.8564 Tiempo/epoch:0.163s\n",
      "Epoch:004 Acc:51.22 Loss:0.7401 Tiempo/epoch:0.160s\n",
      "Epoch:005 Acc:48.78 Loss:0.6952 Tiempo/epoch:0.161s\n",
      "Epoch:006 Acc:48.78 Loss:0.8941 Tiempo/epoch:0.157s\n",
      "Epoch:007 Acc:51.30 Loss:0.6898 Tiempo/epoch:0.154s\n",
      "Epoch:008 Acc:51.22 Loss:0.7423 Tiempo/epoch:0.151s\n",
      "Epoch:009 Acc:48.78 Loss:1.0828 Tiempo/epoch:0.149s\n",
      "Epoch:010 Acc:48.78 Loss:0.7200 Tiempo/epoch:0.148s\n",
      "Epoch:011 Acc:55.46 Loss:0.6862 Tiempo/epoch:0.146s\n",
      "Epoch:012 Acc:58.54 Loss:0.6837 Tiempo/epoch:0.146s\n",
      "Epoch:013 Acc:51.22 Loss:0.7034 Tiempo/epoch:0.145s\n",
      "Epoch:014 Acc:48.84 Loss:0.7164 Tiempo/epoch:0.144s\n",
      "Epoch:015 Acc:51.30 Loss:0.6994 Tiempo/epoch:0.143s\n",
      "Epoch:016 Acc:51.22 Loss:0.7597 Tiempo/epoch:0.143s\n",
      "Epoch:017 Acc:48.78 Loss:0.9168 Tiempo/epoch:0.143s\n",
      "Epoch:018 Acc:48.78 Loss:0.8181 Tiempo/epoch:0.142s\n",
      "Epoch:019 Acc:49.00 Loss:0.7602 Tiempo/epoch:0.142s\n",
      "Epoch:020 Acc:53.92 Loss:0.6904 Tiempo/epoch:0.142s\n",
      "Epoch:021 Acc:50.40 Loss:0.7251 Tiempo/epoch:0.142s\n",
      "Epoch:022 Acc:61.14 Loss:0.6653 Tiempo/epoch:0.143s\n",
      "Epoch:023 Acc:56.54 Loss:0.6777 Tiempo/epoch:0.144s\n",
      "Epoch:024 Acc:54.00 Loss:0.6881 Tiempo/epoch:0.145s\n",
      "Epoch:025 Acc:52.16 Loss:0.7076 Tiempo/epoch:0.147s\n",
      "Epoch:026 Acc:49.36 Loss:0.7718 Tiempo/epoch:0.148s\n",
      "Epoch:027 Acc:51.46 Loss:0.7673 Tiempo/epoch:0.148s\n",
      "Epoch:028 Acc:52.44 Loss:0.7095 Tiempo/epoch:0.149s\n",
      "Epoch:029 Acc:61.34 Loss:0.6603 Tiempo/epoch:0.150s\n",
      "Epoch:030 Acc:59.98 Loss:0.6631 Tiempo/epoch:0.150s\n",
      "Epoch:031 Acc:61.32 Loss:0.6591 Tiempo/epoch:0.150s\n",
      "Epoch:032 Acc:51.44 Loss:0.7971 Tiempo/epoch:0.150s\n",
      "Epoch:033 Acc:58.06 Loss:0.6703 Tiempo/epoch:0.149s\n",
      "Epoch:034 Acc:59.26 Loss:0.6646 Tiempo/epoch:0.149s\n",
      "Epoch:035 Acc:58.94 Loss:0.6650 Tiempo/epoch:0.149s\n",
      "Epoch:036 Acc:59.54 Loss:0.6632 Tiempo/epoch:0.148s\n",
      "Epoch:037 Acc:61.66 Loss:0.6572 Tiempo/epoch:0.148s\n",
      "Epoch:038 Acc:58.88 Loss:0.6657 Tiempo/epoch:0.147s\n",
      "Epoch:039 Acc:61.90 Loss:0.6543 Tiempo/epoch:0.147s\n",
      "Epoch:040 Acc:52.56 Loss:0.7213 Tiempo/epoch:0.147s\n",
      "Epoch:041 Acc:55.98 Loss:0.6887 Tiempo/epoch:0.146s\n",
      "Epoch:042 Acc:54.30 Loss:0.6971 Tiempo/epoch:0.146s\n",
      "Epoch:043 Acc:58.68 Loss:0.6680 Tiempo/epoch:0.145s\n",
      "Epoch:044 Acc:57.36 Loss:0.6748 Tiempo/epoch:0.145s\n",
      "Epoch:045 Acc:52.84 Loss:0.7173 Tiempo/epoch:0.146s\n",
      "Epoch:046 Acc:56.54 Loss:0.6844 Tiempo/epoch:0.145s\n",
      "Epoch:047 Acc:60.28 Loss:0.6571 Tiempo/epoch:0.145s\n",
      "Epoch:048 Acc:55.34 Loss:0.6998 Tiempo/epoch:0.145s\n",
      "Epoch:049 Acc:56.66 Loss:0.6767 Tiempo/epoch:0.144s\n",
      "Epoch:050 Acc:59.62 Loss:0.6602 Tiempo/epoch:0.144s\n",
      "Epoch:051 Acc:64.00 Loss:0.6395 Tiempo/epoch:0.144s\n",
      "Epoch:052 Acc:64.54 Loss:0.6377 Tiempo/epoch:0.144s\n",
      "Epoch:053 Acc:59.86 Loss:0.6590 Tiempo/epoch:0.144s\n",
      "Epoch:054 Acc:60.52 Loss:0.6529 Tiempo/epoch:0.144s\n",
      "Epoch:055 Acc:64.36 Loss:0.6342 Tiempo/epoch:0.144s\n",
      "Epoch:056 Acc:54.98 Loss:0.6896 Tiempo/epoch:0.144s\n",
      "Epoch:057 Acc:63.00 Loss:0.6355 Tiempo/epoch:0.144s\n",
      "Epoch:058 Acc:61.32 Loss:0.6474 Tiempo/epoch:0.143s\n",
      "Epoch:059 Acc:56.16 Loss:0.7016 Tiempo/epoch:0.143s\n",
      "Epoch:060 Acc:67.30 Loss:0.6106 Tiempo/epoch:0.143s\n",
      "Epoch:061 Acc:63.20 Loss:0.6305 Tiempo/epoch:0.143s\n",
      "Epoch:062 Acc:67.86 Loss:0.6035 Tiempo/epoch:0.143s\n",
      "Epoch:063 Acc:54.86 Loss:0.7296 Tiempo/epoch:0.142s\n",
      "Epoch:064 Acc:56.92 Loss:0.6789 Tiempo/epoch:0.142s\n",
      "Epoch:065 Acc:54.50 Loss:0.7030 Tiempo/epoch:0.142s\n",
      "Epoch:066 Acc:57.28 Loss:0.6754 Tiempo/epoch:0.142s\n",
      "Epoch:067 Acc:67.58 Loss:0.5890 Tiempo/epoch:0.142s\n",
      "Epoch:068 Acc:71.38 Loss:0.5630 Tiempo/epoch:0.142s\n",
      "Epoch:069 Acc:48.78 Loss:1.2015 Tiempo/epoch:0.141s\n",
      "Epoch:070 Acc:56.56 Loss:0.6681 Tiempo/epoch:0.141s\n",
      "Epoch:071 Acc:55.56 Loss:0.6818 Tiempo/epoch:0.141s\n",
      "Epoch:072 Acc:48.80 Loss:1.2008 Tiempo/epoch:0.141s\n",
      "Epoch:073 Acc:69.12 Loss:0.5406 Tiempo/epoch:0.141s\n",
      "Epoch:074 Acc:51.12 Loss:0.7721 Tiempo/epoch:0.141s\n",
      "Epoch:075 Acc:51.50 Loss:0.7612 Tiempo/epoch:0.141s\n",
      "Epoch:076 Acc:57.76 Loss:0.6507 Tiempo/epoch:0.141s\n",
      "Epoch:077 Acc:75.26 Loss:0.4808 Tiempo/epoch:0.140s\n",
      "Epoch:078 Acc:77.68 Loss:0.4642 Tiempo/epoch:0.140s\n",
      "Epoch:079 Acc:53.42 Loss:0.7731 Tiempo/epoch:0.140s\n",
      "Epoch:080 Acc:58.24 Loss:0.6536 Tiempo/epoch:0.140s\n",
      "Epoch:081 Acc:78.72 Loss:0.4203 Tiempo/epoch:0.140s\n",
      "Epoch:082 Acc:53.86 Loss:0.8409 Tiempo/epoch:0.140s\n",
      "Epoch:083 Acc:67.56 Loss:0.5100 Tiempo/epoch:0.140s\n",
      "Epoch:084 Acc:77.68 Loss:0.4098 Tiempo/epoch:0.140s\n",
      "Epoch:085 Acc:94.50 Loss:0.2956 Tiempo/epoch:0.140s\n",
      "Epoch:086 Acc:74.24 Loss:0.4329 Tiempo/epoch:0.140s\n",
      "Epoch:087 Acc:92.14 Loss:0.2502 Tiempo/epoch:0.140s\n",
      "Epoch:088 Acc:61.44 Loss:0.6501 Tiempo/epoch:0.140s\n",
      "Epoch:089 Acc:79.80 Loss:0.3618 Tiempo/epoch:0.139s\n",
      "Epoch:090 Acc:66.14 Loss:0.5433 Tiempo/epoch:0.139s\n",
      "Epoch:091 Acc:84.22 Loss:0.3110 Tiempo/epoch:0.139s\n",
      "Epoch:092 Acc:95.98 Loss:0.1800 Tiempo/epoch:0.139s\n",
      "Epoch:093 Acc:92.30 Loss:0.2123 Tiempo/epoch:0.139s\n",
      "Epoch:094 Acc:92.46 Loss:0.1886 Tiempo/epoch:0.139s\n",
      "Epoch:095 Acc:90.56 Loss:0.2182 Tiempo/epoch:0.139s\n",
      "Epoch:096 Acc:84.50 Loss:0.2913 Tiempo/epoch:0.139s\n",
      "Epoch:097 Acc:76.56 Loss:0.3941 Tiempo/epoch:0.139s\n",
      "Epoch:098 Acc:99.80 Loss:0.0588 Tiempo/epoch:0.139s\n",
      "Epoch:099 Acc:99.08 Loss:0.0845 Tiempo/epoch:0.139s\n",
      "Epoch:100 Acc:66.38 Loss:0.6467 Tiempo/epoch:0.139s\n"
     ]
    }
   ],
   "source": [
    "N = 5000 # numero de ejemplos\n",
    "f = 300 # numero de features\n",
    "epochs = 100\n",
    "\n",
    "loop_FFNN_pytorch_style(\n",
    "    dataset,\n",
    "    batch_size=32,  # 10\n",
    "    d1=300,\n",
    "    d2=500,\n",
    "    N=N,\n",
    "    epochs=epochs,\n",
    "    run_in_GPU=True,\n",
    "    lr=0.06,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
